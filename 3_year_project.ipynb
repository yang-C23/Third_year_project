{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "zkrX0hHqqbZr"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-d5df0069828e>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolab\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdrive\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdrive\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'/content/drive'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001B[0m in \u001B[0;36mmount\u001B[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mmount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmountpoint\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mforce_remount\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout_ms\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m120000\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreadonly\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    102\u001B[0m   \u001B[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 103\u001B[0;31m   return _mount(\n\u001B[0m\u001B[1;32m    104\u001B[0m       \u001B[0mmountpoint\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    105\u001B[0m       \u001B[0mforce_remount\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mforce_remount\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001B[0m in \u001B[0;36m_mount\u001B[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001B[0m\n\u001B[1;32m    130\u001B[0m   )\n\u001B[1;32m    131\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mephemeral\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 132\u001B[0;31m     _message.blocking_request(\n\u001B[0m\u001B[1;32m    133\u001B[0m         \u001B[0;34m'request_auth'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrequest\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'authType'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m'dfs_ephemeral'\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout_sec\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    134\u001B[0m     )\n",
      "\u001B[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_message.py\u001B[0m in \u001B[0;36mblocking_request\u001B[0;34m(request_type, request, timeout_sec, parent)\u001B[0m\n\u001B[1;32m    174\u001B[0m       \u001B[0mrequest_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrequest\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparent\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mparent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexpect_reply\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    175\u001B[0m   )\n\u001B[0;32m--> 176\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0mread_reply_from_input\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrequest_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout_sec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_message.py\u001B[0m in \u001B[0;36mread_reply_from_input\u001B[0;34m(message_id, timeout_sec)\u001B[0m\n\u001B[1;32m     94\u001B[0m     \u001B[0mreply\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_read_next_input_message\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     95\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mreply\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0m_NOT_READY\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreply\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 96\u001B[0;31m       \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.025\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     97\u001B[0m       \u001B[0;32mcontinue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     98\u001B[0m     if (\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5FeARV9qjcH"
   },
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqlwHnksqjWR"
   },
   "outputs": [],
   "source": [
    "import  xml.dom.minidom\n",
    "import os\n",
    "import re\n",
    "import linecache\n",
    "from xml.parsers.expat import ExpatError\n",
    "import torch\n",
    "!pip install openprompt\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from openprompt.data_utils import InputExample\n",
    "import random\n",
    "from itertools import zip_longest\n",
    "from random import shuffle\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt import PromptDataLoader\n",
    "!pip install transformers\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from openprompt.prompts import ManualTemplate, MixedTemplate, SoftTemplate, PrefixTuningTemplate\n",
    "from openprompt.prompts import ManualVerbalizer, SoftVerbalizer\n",
    "from openprompt import PromptForClassification\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from openprompt import PromptForClassification\n",
    "import pandas as pd\n",
    "from openprompt.plms import T5TokenizerWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuTh_xoMD2_3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "folder_path = \"/content/drive/MyDrive/3rd_year_project/2012_Temporal_Relations_Challenge/2012-07-15.original-annotation.release\"\n",
    "\n",
    "word_counts = {}\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        # Read the file line by line\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split()\n",
    "                word_count = len(words)\n",
    "                \n",
    "                if word_count in word_counts:\n",
    "                    word_counts[word_count] += 1\n",
    "                else:\n",
    "                    word_counts[word_count] = 1\n",
    "\n",
    "# Sort the word counts by the number of words in a sentence\n",
    "sorted_word_counts = sorted(word_counts.items())\n",
    "\n",
    "# Separate the sorted data into two lists for plotting\n",
    "word_count_values, sentence_counts = zip(*sorted_word_counts)\n",
    "\n",
    "# Create a bar chart\n",
    "plt.bar(word_count_values, sentence_counts)\n",
    "plt.xlabel('Number of words in a sentence')\n",
    "plt.ylabel('Number of sentences')\n",
    "plt.title('Word count distribution in sentences')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dz0IoYgsHkvS"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "#generate gold labels by rule based method\n",
    "def gold_labeling(path, window_size_before_target_sentence, window_size_after_target_sentence):\n",
    "    files = os.listdir(path)\n",
    "    files.sort()\n",
    "\n",
    "    dataset = {}\n",
    "    treatment_list = []\n",
    "    tlink_list = []\n",
    "    admission = \"\"\n",
    "    discharge = \"\"\n",
    "    for file in files:\n",
    "\n",
    "        #extract treatment entities and time information\n",
    "        if file.endswith(\".extent\"):\n",
    "\n",
    "            treatment_list = []\n",
    "\n",
    "            f = open(path+\"/\"+file)\n",
    "            for line in f:\n",
    "                s = re.split(r'[||\"\"\\'\\']',line)\n",
    "                if s[0] == \"EVENT=\":\n",
    "                    if s[5] == \"TREATMENT\":\n",
    "                        treatment_list.append(s[1])\n",
    "                elif s[0] == \"SECTIME=\":\n",
    "                    if s[5] == \"ADMISSION\":\n",
    "                        admission = s[1]\n",
    "                    elif s[5] == \"DISCHARGE\":\n",
    "                        discharge = s[1]\n",
    "\n",
    "        #extract temporal link information\n",
    "        elif file.endswith(\".tlink\"):\n",
    "            tlink_list = []\n",
    "\n",
    "            t = open(path+\"/\"+file)\n",
    "            for line in t:\n",
    "                tlink = re.split(r'[||\"\"\\'\\']',line)\n",
    "                for i in treatment_list:\n",
    "                    if tlink[1] == i:\n",
    "                        begin = [int(x) for x in tlink[2].split()[0].split(\":\")]\n",
    "                        end = [int(y) for y in tlink[2].split()[1].split(\":\")]\n",
    "                        txt_path = (path+\"/\"+file).replace(\".tlink\", \".txt\")\n",
    "                        line_number = begin[0]-1\n",
    "                        txt_content = [x.replace(\"\\n\", \" \") for x in linecache.getlines(txt_path)]\n",
    "                        text_a = txt_content[:4] +[\". Dorctor notes: \"]\n",
    "\n",
    "                        #context window\n",
    "                        #One sentence before: One sentence after\n",
    "                        #[line_number-1 : line_number+2]\n",
    "                        target_sentent = txt_content[line_number - window_size_before_target_sentence : (line_number+1)+window_size_after_target_sentence]\n",
    "                        text_a += target_sentent\n",
    "                        if tlink[5] == admission and tlink[9] == \"BEFORE\":\n",
    "                            tlink_list.append([i, \"OFF\", begin, end, \"\".join(text_a)])\n",
    "                        elif tlink[5] == admission and tlink[9] == \"AFTER\":\n",
    "                            tlink_list.append([i, \"ON\", begin, end, \"\".join(text_a)])\n",
    "                        elif tlink[5] == discharge and tlink[9] == \"BEFORE\":\n",
    "                            tlink_list.append([i, \"ON\", begin, end, \"\".join(text_a)])\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "            dataset[file] = tlink_list\n",
    "            \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_train_set(train_path, number_ON_label, number_OFF_label, window_size_before_target_sentence, window_size_after_target_sentence):\n",
    "    #train_path = \"/content/drive/MyDrive/3rd_year_project/2012_Temporal_Relations_Challenge/2012-07-15.original-annotation.release\"\n",
    "\n",
    "    train_data = gold_labeling(train_path, window_size_before_target_sentence, window_size_after_target_sentence)\n",
    "    #print(train_data)\n",
    "    label_ON_dataset = []\n",
    "    label_OFF_dataset = []\n",
    "\n",
    "    train_dataset_prompt = []\n",
    "    train_dataset_fine = []\n",
    "    count = 0\n",
    "    train_ON_count = 0\n",
    "    train_OFF_count = 0\n",
    "    total_number_ON = 0\n",
    "    total_number_OFF = 0\n",
    "    for file in train_data:\n",
    "\n",
    "        for gold_label in train_data[file]:\n",
    "            if gold_label[1] == \"ON\":\n",
    "                label_ON_dataset.append([file]+gold_label)\n",
    "                total_number_ON+=1\n",
    "            else:\n",
    "                label_OFF_dataset.append([file]+gold_label)\n",
    "                total_number_OFF+=1\n",
    "    \n",
    "    #few-shot training set\n",
    "    if number_ON_label<=total_number_ON :\n",
    "        train_label_ON_dataset = random.sample(label_ON_dataset, number_ON_label)\n",
    "    else :\n",
    "        train_label_ON_dataset = label_ON_dataset\n",
    "    if number_OFF_label<=total_number_OFF:\n",
    "        train_label_OFF_dataset = random.sample(label_OFF_dataset, number_OFF_label)\n",
    "    else :\n",
    "        train_label_OFF_dataset = label_OFF_dataset\n",
    "    \n",
    "    #randomly merge label ON and OFF dataset\n",
    "    print(\"train_label_ON_dataset\",len(train_label_ON_dataset))\n",
    "    print(\"train_label_OFF_dataset\",len(train_label_OFF_dataset))\n",
    "    temp_list = list(zip_longest(train_label_ON_dataset, train_label_OFF_dataset, fillvalue=None))\n",
    "    shuffle(temp_list)\n",
    "\n",
    "    merged_label_dataset = []\n",
    "    for item in temp_list:\n",
    "        if item[0] is not None:\n",
    "            merged_label_dataset.append(item[0])\n",
    "        if item[1] is not None:\n",
    "            merged_label_dataset.append(item[1])\n",
    "\n",
    "    print(len(merged_label_dataset))\n",
    "    for gold_label in merged_label_dataset:\n",
    "        if gold_label[2] == \"ON\":\n",
    "            train_label = 1\n",
    "            train_ON_count += 1\n",
    "        else:\n",
    "            train_label = 0\n",
    "            train_OFF_count += 1\n",
    "\n",
    "        #generate training data for prompt-based leanring\n",
    "        input_example = InputExample(text_a = gold_label[5], text_b = gold_label[1], label = train_label, guid = count, meta = gold_label[0])\n",
    "        count += 1\n",
    "        #generate training data for fine-tuning\n",
    "        train_dataset_prompt.append(input_example)\n",
    "        train_dataset_fine.append([gold_label[5], gold_label[1], train_label, count, gold_label[0]])\n",
    "\n",
    "    print(input_example)\n",
    "    print(\"number of ON train\",train_ON_count)\n",
    "    print(\"number of OFF train\",train_OFF_count)\n",
    "    print(\"total number of training set\", count)\n",
    "\n",
    "    \n",
    "    return (train_dataset_prompt, train_dataset_fine)\n",
    "\n",
    "\n",
    "def get_test_set (test_path, window_size_before_target_sentence, window_size_after_target_sentence):\n",
    "\n",
    "    test_ON_count = 0\n",
    "    test_OFF_count = 0\n",
    "    #test_path = \"/content/drive/MyDrive/3rd_year_project/2012_Temporal_Relations_Challenge/ground_truth/merged_i2b2\"\n",
    "    test_data = gold_labeling(test_path, window_size_before_target_sentence, window_size_after_target_sentence)\n",
    "    #print(test_data)\n",
    "    test_dataset_prompt = []\n",
    "    test_dataset_fine = []\n",
    "    count = 0\n",
    "    for file in test_data:\n",
    "        \n",
    "        #creat training labels\n",
    "        for gold_label in test_data[file]:\n",
    "            if gold_label[1] == \"ON\":\n",
    "                test_label = 1\n",
    "                test_ON_count += 1\n",
    "            else:\n",
    "                test_label = 0\n",
    "                test_OFF_count += 1\n",
    "            \n",
    "            input_example = InputExample(text_a = gold_label[4], text_b = gold_label[0], label = test_label, guid = count, meta = file)\n",
    "            count += 1\n",
    "            test_dataset_prompt.append(input_example)\n",
    "            test_dataset_fine.append([gold_label[4], gold_label[0], test_label, count, file])\n",
    "\n",
    "    print(\"number of ON test\",test_ON_count)\n",
    "\n",
    "    print(\"number of OFF test\",test_OFF_count)\n",
    "\n",
    "    return (test_dataset_prompt, test_dataset_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGlKTXVW75N8"
   },
   "outputs": [],
   "source": [
    " #different pre-trained language model\n",
    "def select_plm (option):\n",
    "    if option == 0:\n",
    "        return load_plm(\"bert\", \"bert-base-uncased\")\n",
    "    elif option == 1:\n",
    "        return load_plm(\"roberta\", \"roberta-base\")\n",
    "    elif option == 2:\n",
    "        return load_plm(\"t5\", \"t5-base\")\n",
    "    elif option == 3:\n",
    "        return load_plm(\"gpt2\", \"gpt2\")\n",
    "    else :\n",
    "        print(\"there is no option \"+ option +\"in PLM selection\")\n",
    "        return None\n",
    "\n",
    "    #return format: plm, tokenizer, model_config, WrapperClass\n",
    "\n",
    "\n",
    "#different template and verbalizer\n",
    "#manual, soft, mixed, prefix\n",
    "def select_template_and_verbalizer (option, tokenizer, plm):\n",
    "    if option == 0:\n",
    "        return (\n",
    "        ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} Question: {\"placeholder\":\"text_b\"} were used between admission and discharge time. Is it correct? {\"mask\"}.'),\n",
    "        ManualVerbalizer(tokenizer, num_classes=2, label_words=[[\"yes\"], [\"no\"]])\n",
    "        )\n",
    "    elif option == 1:\n",
    "        return (\n",
    "        ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} In this paragraph of the note, {\"placeholder\":\"text_b\"} {\"mask\"} used between admission and discharge time.'),\n",
    "        ManualVerbalizer(tokenizer, num_classes=2, label_words=[[\"is\"], [\"not\"]])\n",
    "        )\n",
    "    elif option == 2:\n",
    "        return (\n",
    "        ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} In this paragraph of the note, {\"placeholder\":\"text_b\"} {\"mask\"} used between admission and discharge time.'),\n",
    "        ManualVerbalizer(\n",
    "        tokenizer = tokenizer,\n",
    "        num_classes = 2, \n",
    "        classes = [\"ON\", \"OFF\"],\n",
    "        label_words = {\n",
    "            \"ON\": [\"definitely\"],\n",
    "            \"OFF\": [\"possibly\",\"probably\",\"difinitely not\"],\n",
    "            })\n",
    "        )\n",
    "    elif option == 3:\n",
    "        return (\n",
    "        ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} Question: {\"placeholder\":\"text_b\"} were used between admission and discharge time. Is it correct? {\"mask\"}.'),\n",
    "        SoftVerbalizer(tokenizer, plm, num_classes=2)\n",
    "        )\n",
    "    elif option == 4:\n",
    "        return (\n",
    "        #only for seq2seq, how about altoregressive model.\n",
    "        PrefixTuningTemplate(model=plm, tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} Question: {\"placeholder\":\"text_b\"} were used between admission and discharge time. Is it correct? {\"mask\"}'),\n",
    "        SoftVerbalizer(tokenizer, plm, num_classes=2)\n",
    "        )\n",
    "\n",
    "    elif option == 5:\n",
    "        return (\n",
    "        MixedTemplate(model=plm, tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} {\"soft\": \"Question:\"} {\"placeholder\":\"text_b\"} {\"soft\"} {\"soft\"} {\"soft\"} {\"soft\"} {\"soft\"} Is it correct? {\"mask\"}.'),\n",
    "        ManualVerbalizer(tokenizer, num_classes=2, label_words=[[\"yes\"], [\"no\"]])\n",
    "        )\n",
    "    elif option == 6:\n",
    "        return (\n",
    "        MixedTemplate(model=plm, tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} In this paragraph of the note, {\"placeholder\":\"text_b\"} {\"soft\"} {\"mask\"} {\"soft\"} {\"soft\"} {\"soft\"} {\"soft\"} {\"soft\"} {\"soft\"}'),\n",
    "        SoftVerbalizer(tokenizer, plm, num_classes=2)\n",
    "        )\n",
    "    else:\n",
    "        print(\"we do not have option \"+option)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74CwcUkgGOg_"
   },
   "source": [
    "collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDMg8dzS75JJ"
   },
   "outputs": [],
   "source": [
    "#set training data and testing data\n",
    "#able to use few-shot method\n",
    "train_path = \"/content/drive/MyDrive/3rd_year_project/2012_Temporal_Relations_Challenge/2012-07-15.original-annotation.release\"\n",
    "number_ON_label = 5\n",
    "number_OFF_label = 5\n",
    "window_size_before_target_sentence = 3\n",
    "window_size_after_target_sentence = 2\n",
    "train_dataset_prompt, train_dataset_tuning = get_train_set(train_path, number_ON_label, number_OFF_label, window_size_before_target_sentence, window_size_after_target_sentence)\n",
    "\n",
    "\n",
    "test_path = \"/content/drive/MyDrive/3rd_year_project/2012_Temporal_Relations_Challenge/ground_truth/merged_i2b2\"\n",
    "test_dataset_prompt ,test_dataset_tuning = get_test_set(test_path, window_size_before_target_sentence, window_size_after_target_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUoiyrU8D-t7"
   },
   "outputs": [],
   "source": [
    "# display input informations\n",
    "train_dataframe = pd.DataFrame(train_dataset_tuning , columns=[\"discharge note\", \"treatment entity\", \"label(1-ON,0-OFF)\", \"guid\", \"document name\"])\n",
    "train_dataframe = train_dataframe.reindex(columns=[\"document name\",\"discharge note\",\"treatment entity\", \"label(1-ON,0-OFF)\"])\n",
    "train_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9h-zfcChME5t"
   },
   "outputs": [],
   "source": [
    "test_dataframe = pd.DataFrame(test_dataset_tuning , columns=[\"doctor note\", \"treatment entity\", \"label\", \"guid\", \"document name\"])\n",
    "test_dataframe = test_dataframe.reindex(columns=[\"document name\",\"doctor note\",\"treatment entity\", \"label\"])\n",
    "test_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkGk29ouwLU1"
   },
   "source": [
    " Prompt based learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPyhMymgBIqE"
   },
   "outputs": [],
   "source": [
    "plm, tokenizer, model_config, WrapperClass = select_plm(0)\n",
    "\n",
    "mytemplate, myverbalizer = select_template_and_verbalizer(0, tokenizer, plm)\n",
    "\n",
    "#how dose the template look like\n",
    "wrapped_example = mytemplate.wrap_one_example(train_dataset_prompt[0])\n",
    "print(wrapped_example)\n",
    "\n",
    "text_list = []\n",
    "\n",
    "for item in wrapped_example[0]:\n",
    "    text_list.append(item['text'])\n",
    "\n",
    "df = pd.DataFrame({'text': text_list})\n",
    "df\n",
    "\n",
    "# wrapped_t5tokenizer= T5TokenizerWrapper(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer,truncate_method=\"head\")\n",
    "# tokenized_example = wrapped_t5tokenizer.tokenize_one_example(wrapped_example, teacher_forcing=False)\n",
    "# print(tokenized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UiWBJQoezm68"
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWCZQzDm75Lg"
   },
   "outputs": [],
   "source": [
    "#prompt dataloader\n",
    "\n",
    "train_dataloader_prompt = PromptDataLoader(dataset=train_dataset_prompt, template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method='head')\n",
    "#truncate_method=`head`, `tail`, `balanced`\n",
    "#max_seq_length (int, optional) – The max sequence length of the input ids. It’s used to truncate sentences.\n",
    "\n",
    "validation_dataloader_prompt = PromptDataLoader(dataset=test_dataset_prompt, template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "\n",
    "\n",
    "#training\n",
    "use_cuda = True\n",
    "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
    "if use_cuda:\n",
    "    prompt_model=  prompt_model.cuda()\n",
    "\n",
    "\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
    "\n",
    "for epoch in range(5):\n",
    "    tot_loss = 0\n",
    "    for step, inputs in enumerate(train_dataloader_prompt):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step %100 ==1:\n",
    "            print(\"Epoch {}, average loss: {}\".format(epoch, tot_loss/(step+1)), flush=True)\n",
    "\n",
    "\n",
    "#evaluation\n",
    "allpreds = []\n",
    "alllabels = []\n",
    "allids = []\n",
    "for step, inputs in enumerate(validation_dataloader_prompt):\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = prompt_model(inputs)\n",
    "    guids = inputs['guid']\n",
    "    labels = inputs['label']\n",
    "    alllabels.extend(labels.cpu().tolist())\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "    allids.extend(guids.cpu().tolist())\n",
    "\n",
    "accuracy = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "\n",
    "\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "errorIDs = []\n",
    "count = 0\n",
    "for i,j,id in zip(allpreds, alllabels, allids):\n",
    "    count += 1\n",
    "    if j == 1:\n",
    "        if i == 1:\n",
    "            TP += 1\n",
    "        else:\n",
    "            FN += 1\n",
    "            errorIDs.append(id)\n",
    "    else:\n",
    "        if i == 1:\n",
    "            FP += 1\n",
    "            errorIDs.append(id)\n",
    "        else:\n",
    "            TN += 1\n",
    "\n",
    "print(TP, FN, FP, TN)\n",
    "#confusion matrix\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "F1 = 2*(precision*recall)/(precision + recall)\n",
    "\n",
    "\n",
    "print(\"overall accuracy:\",accuracy)\n",
    "print(\"ON accuracy:     \",(TP)/(TP+FN))\n",
    "print(\"OFF accuracy:    \",(FP)/(FP+TN))\n",
    "print(\"Precision:       \",precision)\n",
    "print(\"Recall:          \",recall)\n",
    "print(\"F1:              \",F1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TlKSRpjrw7oP"
   },
   "outputs": [],
   "source": [
    "#error analysis\n",
    "error_inputs = [x for x in test_dataset_prompt if x.guid in errorIDs]\n",
    "# print(error_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGMbxFWKFX3e"
   },
   "source": [
    "fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3nTgXe6vEhT_"
   },
   "outputs": [],
   "source": [
    "#load tokenizer for fine-tuning plm, and set the maximum sequence length \n",
    "tokenizer_tuning = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "for dataset in train_dataset_tuning:\n",
    "    input = dataset[0]+'Treament: '+dataset[1]\n",
    "    input_ids = tokenizer_tuning.encode(input, add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "print(input)        \n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCHmExX0E8NW"
   },
   "source": [
    "这里的‘ ：’格式有点奇怪，句号都是在空格后"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yylPw-dFE5T"
   },
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = [i[2] for i in train_dataset_tuning]\n",
    "for dataset in train_dataset_tuning:\n",
    "    input = dataset[0]+'Treatment: '+dataset[1]\n",
    "    encoded_dict = tokenizer_tuning.encode_plus(\n",
    "                    input,                      # Sentence to encode.\n",
    "                    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                    max_length = 276,           # Pad & truncate all sentences.\n",
    "                    pad_to_max_length = True,\n",
    "                    return_attention_mask = True,   # Construct attn. masks.\n",
    "                    return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "\n",
    "\n",
    "####set the training and validation set####\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "\n",
    "####dataloader####\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "model.cuda()\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 5\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "\n",
    "\n",
    "#Evaluation\n",
    "\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "test_labels = [i[2] for i in test_dataset_tuning]\n",
    "\n",
    "# For every sentence...\n",
    "for dataset in test_dataset_tuning:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    input = dataset[0]+ 'Treatment: '+dataset[1]\n",
    "\n",
    "    encoded_dict = tokenizer_tuning.encode_plus(\n",
    "                        input,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 276,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(test_labels)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels, pred_labels = [], [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "  with torch.no_grad():\n",
    "      result = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     return_dict=True)\n",
    "\n",
    "  logits = result.logits\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "\n",
    "  #predictions.append(logits)\n",
    "  pred_labels.extend(np.argmax(logits, axis=1).flatten())\n",
    "  \n",
    "  true_labels.extend(label_ids)\n",
    "\n",
    "\n",
    "accuracy = sum([int(i==j) for i,j in zip(pred_labels, true_labels)])/len(pred_labels)\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "count = 0\n",
    "for i,j, in zip(pred_labels, true_labels):\n",
    "    count += 1\n",
    "    if j == 1:\n",
    "        if i == 1:\n",
    "            TP += 1\n",
    "        else:\n",
    "            FN += 1\n",
    "    else:\n",
    "        if i == 1:\n",
    "            FP += 1\n",
    "        else:\n",
    "            TN += 1\n",
    "\n",
    "#\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "F1 = 2*(precision*recall)/(precision + recall)\n",
    "\n",
    "\n",
    "print(\"overall accuracy:\",accuracy)\n",
    "print(\"ON accuracy:     \",(TP)/(TP+FN))\n",
    "print(\"OFF accuracy:    \",(TN)/(FP+TN))\n",
    "print(\"Precision:       \",precision)\n",
    "print(\"Recall:          \",recall)\n",
    "print(\"F1:              \",F1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLnWXeEhxbQ7"
   },
   "source": [
    "graph and compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5CwPlJXLXFy"
   },
   "outputs": [],
   "source": [
    "#compare models and paradigm\n",
    "number_of_training = [5, 10, 20 ,40]\n",
    "number_of_template = [1, 6]\n",
    "accuracy_list_prompt = []\n",
    "accuracy_list_tuning = []\n",
    "f1_list_prompt = []\n",
    "f1_list_tuning = []\n",
    "\n",
    "for i in number_of_training:\n",
    "    train_path = \"/content/drive/MyDrive/3rd_year_project/2012_Temporal_Relations_Challenge/2012-07-15.original-annotation.release\"\n",
    "    number_ON_label = i\n",
    "    number_OFF_label = i\n",
    "    window_size_before_target_sentence = 2\n",
    "    window_size_after_target_sentence = 3\n",
    "    train_dataset_prompt, train_dataset_tuning = get_train_set(train_path, number_ON_label, number_OFF_label, window_size_before_target_sentence, window_size_after_target_sentence)\n",
    "\n",
    "    test_path = \"/content/drive/MyDrive/3rd_year_project/2012_Temporal_Relations_Challenge/ground_truth/merged_i2b2\"\n",
    "    test_dataset_prompt ,test_dataset_tuning = get_test_set(test_path, window_size_before_target_sentence, window_size_after_target_sentence)\n",
    "\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = [i[2] for i in train_dataset_tuning]\n",
    "    for dataset in train_dataset_tuning:\n",
    "        input = dataset[0]+'Treatment: '+dataset[1]\n",
    "        encoded_dict = tokenizer_tuning.encode_plus(\n",
    "                        input,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 276,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "    # Create a 90-10 train-validation split.\n",
    "\n",
    "    # Calculate the number of samples to include in each set.\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "\n",
    "    # Divide the dataset by randomly selecting samples.\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    print('{:>5,} training samples'.format(train_size))\n",
    "    print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "\n",
    "    batch_size = 4\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,  # The training samples.\n",
    "                sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "                batch_size = batch_size # Trains with this batch size.\n",
    "            )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "                val_dataset, # The validation samples.\n",
    "                sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "                batch_size = batch_size # Evaluate with this batch size.\n",
    "            )\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                        # You can increase this for multi-class tasks.   \n",
    "        output_attentions = False, # Whether the model returns attentions weights.\n",
    "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    )\n",
    "    model.cuda()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                    )\n",
    "\n",
    "    # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "    # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "    # training data.\n",
    "    epochs = 5\n",
    "\n",
    "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                num_training_steps = total_steps)\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    seed_val = 42\n",
    "\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                \n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            model.zero_grad()        \n",
    "\n",
    "            result = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels,\n",
    "                        return_dict=True)\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "            \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                result = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels,\n",
    "                            return_dict=True)\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "            # Accumulate the validation loss.\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "            \n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        \n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    test_labels = [i[2] for i in test_dataset_tuning]\n",
    "\n",
    "    # For every sentence...\n",
    "    for dataset in test_dataset_tuning:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        input = dataset[0]+ 'Treatment: '+dataset[1]\n",
    "\n",
    "        encoded_dict = tokenizer_tuning.encode_plus(\n",
    "                            input,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 276,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(test_labels)\n",
    "\n",
    "    batch_size = 4\n",
    "\n",
    "    prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    # Prediction on test set\n",
    "\n",
    "    print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    predictions , true_labels, pred_labels = [], [], []\n",
    "\n",
    "    # Predict \n",
    "    for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and \n",
    "        # speeding up prediction\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            result = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask,\n",
    "                            return_dict=True)\n",
    "\n",
    "        logits = result.logits\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Store predictions and true labels\n",
    "\n",
    "        #predictions.append(logits)\n",
    "        pred_labels.extend(np.argmax(logits, axis=1).flatten())\n",
    "\n",
    "        true_labels.extend(label_ids)\n",
    "\n",
    "\n",
    "    accuracy = sum([int(i==j) for i,j in zip(pred_labels, true_labels)])/len(pred_labels)\n",
    "\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    count = 0\n",
    "    for i,j, in zip(pred_labels, true_labels):\n",
    "        count += 1\n",
    "        if j == 1:\n",
    "            if i == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        else:\n",
    "            if i == 1:\n",
    "                FP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "\n",
    "    precision = TP/(TP+FP)#预测为正类中有多少是label正的\n",
    "    recall = TP/(TP+FN)#label正的中有多少被预测出来\n",
    "    F1 = 2*(precision*recall)/(precision + recall)\n",
    "\n",
    "\n",
    "    print(\"overall accuracy:\",accuracy)\n",
    "    print(\"F1:              \",F1)\n",
    "    f1_list_tuning.append(F1)\n",
    "    accuracy_list_tuning.append(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    temp_f1_prompt = []\n",
    "    temp_acc_prompt = []\n",
    "    for j in number_of_template:\n",
    "\n",
    "\n",
    "        plm, tokenizer, model_config, WrapperClass = select_plm(2)\n",
    "\n",
    "        mytemplate, myverbalizer = select_template_and_verbalizer(j, tokenizer, plm)\n",
    "        train_dataloader_prompt = PromptDataLoader(dataset=train_dataset_prompt, template=mytemplate, tokenizer=tokenizer,\n",
    "            tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "            batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "            truncate_method=\"head\")\n",
    "\n",
    "        validation_dataloader_prompt = PromptDataLoader(dataset=test_dataset_prompt, template=mytemplate, tokenizer=tokenizer,\n",
    "            tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "            batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "            truncate_method=\"head\")\n",
    "\n",
    "        use_cuda = True\n",
    "        prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
    "        if use_cuda:\n",
    "            prompt_model=  prompt_model.cuda()\n",
    "\n",
    "\n",
    "        from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
    "\n",
    "        for epoch in range(5):\n",
    "            tot_loss = 0\n",
    "            for step, inputs in enumerate(train_dataloader_prompt):\n",
    "                if use_cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                logits = prompt_model(inputs)\n",
    "                labels = inputs['label']\n",
    "                loss = loss_func(logits, labels)\n",
    "                loss.backward()\n",
    "                tot_loss += loss.item()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                #if step %100 ==1:\n",
    "                    #print(\"Epoch {}, average loss: {}\".format(epoch, tot_loss/(step+1)), flush=True)\n",
    "\n",
    "        # Evaluate\n",
    "\n",
    "        allpreds = []\n",
    "        alllabels = []\n",
    "        allids = []\n",
    "        for step, inputs in enumerate(validation_dataloader_prompt):\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            logits = prompt_model(inputs)\n",
    "            guids = inputs['guid']\n",
    "            labels = inputs['label']\n",
    "            alllabels.extend(labels.cpu().tolist())\n",
    "            allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "            allids.extend(guids.cpu().tolist())\n",
    "\n",
    "        acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "        temp_acc_prompt.append(acc)\n",
    "        print(acc)\n",
    "\n",
    "        #confusion matrix\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        errorIDs = []\n",
    "        count = 0\n",
    "        for i,j,id in zip(allpreds, alllabels, allids):\n",
    "            count += 1\n",
    "            if j == 1:\n",
    "                if i == 1:\n",
    "                    TP += 1\n",
    "                else:\n",
    "                    FN += 1\n",
    "                    errorIDs.append(id)\n",
    "            else:\n",
    "                if i == 1:\n",
    "                    FP += 1\n",
    "                    errorIDs.append(id)\n",
    "                else:\n",
    "                    TN += 1\n",
    "\n",
    "\n",
    "        precision = TP/(TP+FP)\n",
    "        recall = TP/(TP+FN)\n",
    "        F1 = 2*(precision*recall)/(precision + recall)\n",
    "        # print(\"overall accuracy:\",accuracy)\n",
    "        # print(\"ON accuracy:     \",(TP)/(TP+FN))\n",
    "        # print(\"OFF accuracy:    \",(FP)/(FP+TN))\n",
    "        # print(\"Precision:       \",precision)\n",
    "        # print(\"Recall:          \",recall)\n",
    "        print(\"F1:\",F1)\n",
    "        temp_f1_prompt.append(F1)\n",
    "    f1_list_prompt.append(temp_f1_prompt)\n",
    "    accuracy_list_prompt.append(temp_acc_prompt)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-QdL_-nF0QD"
   },
   "outputs": [],
   "source": [
    "print(accuracy_list_prompt)\n",
    "template_a_acc = [i[0] for i in accuracy_list_prompt]\n",
    "template_b_acc = [i[1] for i in accuracy_list_prompt]\n",
    "\n",
    "template_a_f1 = [i[0] for i in f1_list_prompt]\n",
    "template_b_f1 = [i[1] for i in f1_list_prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJ2h_MTuOblS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8Z0JyGYOD7-"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "plt.plot(number_of_training, accuracy_list_tuning, label='fine-tuning')\n",
    "plt.plot(number_of_training, template_a_acc, label='template 1')\n",
    "plt.plot(number_of_training, template_b_acc, label='template 6') \n",
    "plt.title(\"few shot learning with accuracy\")\n",
    "plt.xlabel(\"number of data in each label\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bH0dTYspOtSc"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "plt.plot(number_of_training, f1_list_tuning, label='fine-tuning')\n",
    "plt.plot(number_of_training, template_a_f1, label='template 1')\n",
    "plt.plot(number_of_training, template_b_f1, label='template 6')\n",
    "plt.title(\"few shot learning with F1 score\")\n",
    "plt.xlabel(\"number of data in each label\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgDW1kpZLYrt"
   },
   "outputs": [],
   "source": [
    "#compare models and paradigm\n",
    "number_of_training = [5, 10, 20 ,40, 80, 160, 320, 762]\n",
    "number_of_plm = [0,1]\n",
    "accuracy_list_prompt = []\n",
    "accuracy_list_tuning = []\n",
    "f1_list_prompt = []\n",
    "f1_list_tuning = []\n",
    "\n",
    "for i in number_of_training:\n",
    "    train_path = \"/content/drive/MyDrive/3rd_year_project/2012_Temporal_Relations_Challenge/2012-07-15.original-annotation.release\"\n",
    "    number_ON_label = i\n",
    "    number_OFF_label = i\n",
    "    window_size_before_target_sentence = 1\n",
    "    window_size_after_target_sentence = 1\n",
    "    train_dataset_prompt, train_dataset_tuning = get_train_set(train_path, number_ON_label, number_OFF_label, window_size_before_target_sentence, window_size_after_target_sentence)\n",
    "\n",
    "    test_path = \"/content/drive/MyDrive/3rd_year_project/2012_Temporal_Relations_Challenge/ground_truth/merged_i2b2\"\n",
    "    test_dataset_prompt ,test_dataset_tuning = get_test_set(test_path, window_size_before_target_sentence, window_size_after_target_sentence)\n",
    "\n",
    "\n",
    "\n",
    "    temp_f1_prompt = []\n",
    "    temp_acc_prompt = []\n",
    "    for j in number_of_plm:\n",
    "\n",
    "\n",
    "        plm, tokenizer, model_config, WrapperClass = select_plm(j)\n",
    "\n",
    "        mytemplate, myverbalizer = select_template_and_verbalizer(0, tokenizer, plm)\n",
    "        train_dataloader_prompt = PromptDataLoader(dataset=train_dataset_prompt, template=mytemplate, tokenizer=tokenizer,\n",
    "            tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "            batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "            truncate_method=\"head\")\n",
    "\n",
    "        validation_dataloader_prompt = PromptDataLoader(dataset=test_dataset_prompt, template=mytemplate, tokenizer=tokenizer,\n",
    "            tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "            batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "            truncate_method=\"head\")\n",
    "\n",
    "        use_cuda = True\n",
    "        prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
    "        if use_cuda:\n",
    "            prompt_model=  prompt_model.cuda()\n",
    "\n",
    "\n",
    "        from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
    "\n",
    "        for epoch in range(5):\n",
    "            tot_loss = 0\n",
    "            for step, inputs in enumerate(train_dataloader_prompt):\n",
    "                if use_cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                logits = prompt_model(inputs)\n",
    "                labels = inputs['label']\n",
    "                loss = loss_func(logits, labels)\n",
    "                loss.backward()\n",
    "                tot_loss += loss.item()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                #if step %100 ==1:\n",
    "                    #print(\"Epoch {}, average loss: {}\".format(epoch, tot_loss/(step+1)), flush=True)\n",
    "\n",
    "        # Evaluate\n",
    "\n",
    "        allpreds = []\n",
    "        alllabels = []\n",
    "        allids = []\n",
    "        for step, inputs in enumerate(validation_dataloader_prompt):\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            logits = prompt_model(inputs)\n",
    "            guids = inputs['guid']\n",
    "            labels = inputs['label']\n",
    "            alllabels.extend(labels.cpu().tolist())\n",
    "            allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "            allids.extend(guids.cpu().tolist())\n",
    "\n",
    "        acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "        temp_acc_prompt.append(acc)\n",
    "        print(acc)\n",
    "\n",
    "        #confusion matrix\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        errorIDs = []\n",
    "        count = 0\n",
    "        for i,j,id in zip(allpreds, alllabels, allids):\n",
    "            count += 1\n",
    "            if j == 1:\n",
    "                if i == 1:\n",
    "                    TP += 1\n",
    "                else:\n",
    "                    FN += 1\n",
    "                    errorIDs.append(id)\n",
    "            else:\n",
    "                if i == 1:\n",
    "                    FP += 1\n",
    "                    errorIDs.append(id)\n",
    "                else:\n",
    "                    TN += 1\n",
    "\n",
    "        \n",
    "        precision = TP/(TP+FP)\n",
    "        recall = TP/(TP+FN)\n",
    "        F1 = 2*(precision*recall)/(precision + recall)\n",
    "        print(\"overall accuracy:\",acc)\n",
    "        print(\"ON accuracy:     \",(TP)/(TP+FN))\n",
    "        print(\"OFF accuracy:    \",(TN)/(FP+TN))\n",
    "        print(\"Precision:       \",precision)\n",
    "        print(\"Recall:          \",recall)\n",
    "        print(\"F1:\",F1)\n",
    "        temp_f1_prompt.append(F1)\n",
    "    f1_list_prompt.append(temp_f1_prompt)\n",
    "    accuracy_list_prompt.append(temp_acc_prompt)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdsympw6Orbp"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(accuracy_list_prompt)\n",
    "print(f1_list_prompt)\n",
    "template_a_acc = [i[0] for i in accuracy_list_prompt]\n",
    "template_b_acc = [i[1] for i in accuracy_list_prompt]\n",
    "template_a_f1 = [i[0] for i in f1_list_prompt]\n",
    "template_b_f1 = [i[1] for i in f1_list_prompt]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "plt.plot(number_of_training[:-1], template_a_acc, label='BERT accuracy')\n",
    "plt.plot(number_of_training[:-1], template_b_acc, label='RoBERT accuracy') \n",
    "plt.title(\"few shot learning with accuracy\")\n",
    "plt.xlabel(\"number of data in each label\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "plt.plot(number_of_training[:-1], template_a_f1, label='BERT F1')\n",
    "plt.plot(number_of_training[:-1], template_b_f1, label='RoBERT F1') \n",
    "plt.title(\"few shot learning with F1\")\n",
    "plt.xlabel(\"number of data in each label\")\n",
    "plt.ylabel(\"F1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOFdUSfOk88MHThFl5R9PHU",
   "name": "",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}